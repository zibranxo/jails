# ğŸ” JAILS: Jailbreak Instruction Leakage Detection System

**JAILS** is a hybrid detection system designed to identify **jailbreak and prompt-injection attempts** against Large Language Models (LLMs).
It combines **semantic similarity, linguistic analysis, pattern-based heuristics, and classical machine-learning models** to detect adversarial prompts beyond simple keyword matching.

This project focuses on **prompt-level security analysis** and is intended as a **research and engineering prototype** for LLM safety exploration.

---

## ğŸ¯ Problem Overview

LLMs are vulnerable to a wide range of prompt-based attacks, including:

- Instruction overrides (e.g., *â€œignore all previous instructionsâ€*)
- Role-play jailbreaks (e.g., *â€œact as an unrestricted AIâ€*)
- Paraphrased attacks that evade keyword filters
- Context flooding through repeated adversarial instructions
- Privilege escalation or authority-based manipulation

Rule-based filters alone fail to generalize.  
**JAILS** addresses this by aggregating multiple weak signals into a stronger detection decision.

---

## ğŸ—ï¸ System Architecture

```
Input Prompt (+ optional context)
        â†“
Feature Extraction Layer
- Linguistic & structural features
- Pattern-based manipulation signals
- Statistical text representations
        â†“
Detection Engine
- ML classifier (Gradient Boosting / RF / LR)
- Anomaly detection (LOF)
- Rule-based fallback
        â†“
Output
SAFE / JAILBREAK
+ confidence & risk score
```

---

## âœ¨ Key Capabilities

### ğŸ” Multi-Layer Detection
- Semantic similarity against known jailbreak patterns
- Linguistic features: repetition, length, structure, readability cues
- Pattern matching: instruction overrides, role-play triggers, coercion
- Statistical signals: TF-IDF similarity, clustering, anomaly detection

### ğŸ›ï¸ Machine-Learning Pipeline
- Feature scaling and preprocessing
- Supervised classifiers:
  - Gradient Boosting (primary)
  - Random Forest / Logistic Regression (baseline comparisons)
- Local Outlier Factor (LOF) for unseen / zero-day attacks
- Heuristic fallback when confidence is low

---

## ğŸŒ Attack Coverage (Examples)

| Category | Example | Detection Signal |
|--------|--------|------------------|
| Instruction Override | "Ignore previous rules" | Regex + semantics |
| Role-Play Jailbreak | "Act as DAN" | Patterns + structure |
| Context Flooding | Repeated adversarial text | Repetition + anomaly |
| Privilege Escalation | "Admin override enabled" | Patterns |
| Disguised Intent | "For academic research onlyâ€¦" | Linguistic + semantic |

---

## ğŸ“‚ Repository Structure

```
jails/
â”œâ”€â”€ flooding2.py          # Main detection engine & training logic
â”œâ”€â”€ multi10.py            # Feature extraction & pattern analysis
â”œâ”€â”€ classifier/
â”‚   â””â”€â”€ models/           # Trained model artifacts (joblib)
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## ğŸš€ Quick Start

### 1ï¸âƒ£ Installation
```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ Run the detector
```bash
python flooding2.py
```

The script demonstrates:
- Model training
- Evaluation metrics
- Sample prompt classification

---

## ğŸ“ˆ Model Outputs

For each prompt, JAILS produces:
- Classification: `SAFE` or `JAILBREAK`
- Confidence score
- Risk / threat level
- Feature-based reasoning signals

This makes the system **interpretable**, not a black box.

---

## ğŸ§  What This Project Demonstrates

- Understanding of LLM jailbreak & prompt-injection risks
- Hybrid detection (rules + ML + statistics)
- Feature engineering for NLP security
- Anomaly detection for unseen attacks
- Research-style experimentation with classifiers

---

## âš ï¸ Disclaimer

This project is an **experimental prototype** intended for **learning and research purposes**.
It is **not a production-ready security system**.

---

## ğŸ“„ License

MIT License
